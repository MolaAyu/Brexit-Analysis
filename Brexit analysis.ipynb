{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Political stance classification\n",
    "In this project, we will solve a supervised machine learning task. The data that we will use for training and evaluation will be annotated collectively by us and other individuals.\n",
    "The machine learning task that will be addressed in this project is to develop a text classifier that determines whether a given textual comment expresses an opinion that is positive or negative towards Brexit: the United Kingdom leaving the European Union.\n",
    "The first two parts of this project deal with data annotation and are solved individually. In the third and final part, you will implement the classification system, and here you will work in a group of two or three people.\n",
    "Didactic purpose of this assignment:\n",
    "* Getting some practical understanding of annotating data and inter-annotator agreement.\n",
    "* Practice several aspects of system development based on machine learning: getting data, cleaning data, processing and selecting features, selecting and tuning a model, evaluating.\n",
    "* Analyzing results in a machine learning experiment.\n",
    "Part 1: Crowdsourcing the data\n",
    "We collect at least 100 Brexit-related comments from social media or the comment fields from online articles. Good places to trawl for comments include social media sites such as Youtube, and newspaper sites in Britain and elsewhere, such as the Telegraph, the Guardian, Daily Mail, the Independent, the Sun, Daily Express, Breitbart, Huffington Post or other English-language sources.\n",
    "Collect comments that express a pro- or anti-Brexit stance. We will create a balanced dataset, so you should try to collect about 50 instances of each stance. We did not include comments not expressing an opinion about Brexit. Also, since other annotators will see each comment in isolation, don't include comments where you need to read previous comments to understand the opinion (e.g. \"You're wrong!\"). Try to select comments from a variety of sources.\n",
    "Store all the comments you collected in an Excel file. This file should have two columns. The first column will store your annotation of whether this comment is pro-Brexit (represented as 1 in the spreadsheet) or anti-Brexit (0 in the spreadsheet). The second column should store the text of the comment. Make sure that the text of each comment is stored in a single cell. The following figure shows an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# the actual classification algorithm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# for converting training and test datasets into matrices\n",
    "# TfidfVectorizer does this specifically for documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# for bundling the vectorizer and the classifier as a single \"package\"\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# for splitting the dataset into training and test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for evaluating the quality of the classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# we can choose differnt classifier to model our algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    " \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we wrote a function to read the dataset. This function returns a list of documents X with their corresponding sentiment labels Y. Since we have an array of labels(value) for each row, we use some assumptions to select a label. The array size is either 1 or two or more. For example, if the array size is 1, then check whether it is '1' or '0' and assign it to that particular row. If, the array size is two we have two scenarios, 1st if both annotations are the same then simply we can assign it to its corresponding comment. 2nd if the annotation is '1' and '-1' we should remove that line(row) because it is ambiguous comment(let us check mathematically, count of 1's is 1 and len(count)/2 is also 1, 1> 1 not true, this means we should remove the comment). The same is true for '1' and '0', '0' and '-1'...etc. Let's see if the size of an array is 3, in this case, the majority vote will be assigned to the comment. For example, if we have '0/0/-1', we have two annotators with '0' and one with '-1', so '0' could be assigned to the comment. Finally, we count the number of annotators with Pro-Brexit, Anti-Brexit and Neither pro or anti-Brexit annotations from the training data set. Finally, the function returns a single label for each row for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep=\"\\t\",names=['target', 'comments'],skiprows=0)\n",
    "    \n",
    "    labels = data['target'].astype('str').values\n",
    "    removeRows = []\n",
    "    count_pos=0\n",
    "    count_neg=0\n",
    "    count_none=0\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = labels[i].replace('/','')\n",
    "        \n",
    "        if(labels[i].count('1') > len(labels[i])/2):\n",
    "            labels[i] = 1\n",
    "            count_pos+=1\n",
    "        elif(labels[i].count('0') > len(labels[i])/2):\n",
    "            labels[i] = 0\n",
    "            count_neg+=1\n",
    "        else: \n",
    "            removeRows.append(i)\n",
    "            count_none+=1\n",
    "    data = data.replace(data['target'].values, labels)\n",
    "    print('Positive count:',count_pos)\n",
    "    print('Negative count:',count_neg)\n",
    "    print('None count:',count_none)\n",
    "    #drop ambigous answers\n",
    "    for i in range(len(removeRows)):\n",
    "        data = data.drop([removeRows[i]])\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data returns each row with a single label based on the model we developed shown in the above line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From preliminary data set ......\n",
      "Positive count: 466\n",
      "Negative count: 428\n",
      "None count: 0\n",
      "From first annoation round ......\n",
      "Positive count: 3492\n",
      "Negative count: 3460\n",
      "None count: 0\n",
      "From the final trianing set ......\n",
      "Positive count: 6378\n",
      "Negative count: 5434\n",
      "None count: 1705\n",
      "From the final test set ......\n",
      "Positive count: 616\n",
      "Negative count: 544\n",
      "None count: 0\n"
     ]
    }
   ],
   "source": [
    "# working with preliminary sample set\n",
    "print(\"From preliminary data set ......\")\n",
    "df_preliminay_sample= read_data(\"a2_first_sample.tsv\")\n",
    "# working with first annotation round traing data\n",
    "print(\"From first annoation round ......\")\n",
    "df_first_training_anota= read_data(\"a2a_train_round1.tsv\")\n",
    "# working with the final training \n",
    "print(\"From the final trianing set ......\")\n",
    "df_traing = read_data(\"a2a_train_final.tsv\")\n",
    "print(\"From the final test set ......\")\n",
    "df_test = read_data(\"a2a_test_final.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsample= df_preliminay_sample['comments']\n",
    "Ysample= df_preliminay_sample['target']\n",
    "\n",
    "Xround1= df_first_training_anota['comments']\n",
    "Yround1= df_first_training_anota['target']\n",
    "\n",
    "Xfinal= df_traing['comments']\n",
    "Yfinal= df_traing['target']\n",
    "\n",
    "X_sample1_train, X_sample1_test, Y_sample1_train, Y_sample1_test = train_test_split(Xsample, Ysample, test_size=0.2, random_state=12345)\n",
    "X_round1_train, X_round1_test, Y_round1_train, Y_round1_test = train_test_split(Xround1, Yround1, test_size=0.2, random_state=12345)\n",
    "X_final_train, X_final_test, Y_final_train, Y_final_test = train_test_split(Xfinal, Yfinal, test_size=0.2, random_state=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_final_train=Y_final_train.astype(int)\n",
    "Y_final_test = Y_final_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1391, 2363)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_sample1_test.size,Y_round1_test.size,Y_final_test.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function builds a Pipeline for document classification, consisting of a vectorizer and a classifier. The TfidfVectorizer is used to convert a document collection into a matrix that can be used with scikit learn learning algorithms. (Here are some additional details.) We can choose the classifiers. For now, LinearSVC is a type of linear classifier, which often tends to work quite well for high-dimensional feature spaces.\n",
    "\n",
    "After combining the vectorizer and the classifier into a Pipeline, we call fit to train the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_document_classifier(X,Y):\n",
    "    pipeline = make_pipeline(TfidfVectorizer(), \n",
    "                            LinearSVC()\n",
    "                            #DecisionTreeClassifier()\n",
    "                            #MLPClassifier(alpha=1)\n",
    "                            #SGDClassifier(loss=\"log\", max_iter=50)\n",
    "\n",
    "                            )\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit each datasets using train_document_classifier method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sample = train_document_classifier(X_sample1_train, Y_sample1_train)\n",
    "clf_round1 = train_document_classifier(X_round1_train, Y_round1_train)\n",
    "clf_final = train_document_classifier(X_final_train, Y_final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score from the sample set: 0.659217877094972\n",
      "The accuracy score from the first round dataset: 0.7268152408339325\n",
      "The accuracy score from the second/final round dataset: 0.7917900973338976\n"
     ]
    }
   ],
   "source": [
    "Yguess0 = clf_sample.predict(X_sample1_test)\n",
    "Yguess1 = clf_round1.predict(X_round1_test)\n",
    "Yguess2 = clf_final.predict(X_final_test)\n",
    "\n",
    "accScore0 = accuracy_score(Y_sample1_test, Yguess0)\n",
    "accScore1 = accuracy_score(Y_round1_test, Yguess1)\n",
    "accScore2 = accuracy_score(Y_final_test, Yguess2)\n",
    "\n",
    "print(\"The accuracy score from the sample set:\", accScore0)\n",
    "print(\"The accuracy score from the first round dataset:\", accScore1)\n",
    "print(\"The accuracy score from the second/final round dataset:\", accScore2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the cross-validation and confusion matrix for each data set and check how many are labeled correctly and how many are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Cross_validation for the first round------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mola/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/mola/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.02984643, 0.01979589, 0.02002573]), 'score_time': array([0.00746918, 0.00748563, 0.00726318]), 'test_score': array([0.69456067, 0.63598326, 0.62025316]), 'train_score': array([0.99369748, 1.        , 0.9916318 ])}\n",
      "-----Cross_validation for the first round------\n",
      "{'fit_time': array([0.16993976, 0.15780473, 0.16638851]), 'score_time': array([0.06376386, 0.06249571, 0.06092954]), 'test_score': array([0.71790723, 0.72545847, 0.72638964]), 'train_score': array([0.97275425, 0.96897761, 0.96952535])}\n",
      "-----Cross_validation for the final trining set------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mola/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.29655957, 0.2737844 , 0.27260184]), 'score_time': array([0.1049149 , 0.10553718, 0.11014605]), 'test_score': array([0.76920635, 0.76571429, 0.76849794]), 'train_score': array([0.96856644, 0.96491507, 0.96936508])}\n",
      "-----Confusion Matrix for preliminary sample training and test set------\n",
      "[[48 36]\n",
      " [25 70]]\n",
      "-----Confusion Matrix for round 1 training and test set------\n",
      "[[500 185]\n",
      " [195 511]]\n",
      "-----Confusion Matrix for the final trainin and tset set------\n",
      "[[ 821  257]\n",
      " [ 235 1050]]\n"
     ]
    }
   ],
   "source": [
    "print('-----Cross_validation for the first round------')\n",
    "print(cross_validate(clf_sample, X_sample1_train, Y_sample1_train))\n",
    "\n",
    "print('-----Cross_validation for the first round------')\n",
    "print(cross_validate(clf_round1, X_round1_train, Y_round1_train))\n",
    "\n",
    "print('-----Cross_validation for the final trining set------')\n",
    "print(cross_validate(clf_final, X_final_train, Y_final_train))\n",
    "\n",
    "\n",
    "print('-----Confusion Matrix for preliminary sample training and test set------')\n",
    "print(confusion_matrix(Y_sample1_test, Yguess0))\n",
    "\n",
    "print('-----Confusion Matrix for round 1 training and test set------')\n",
    "print(confusion_matrix(Y_round1_test, Yguess1))\n",
    "\n",
    "print('-----Confusion Matrix for the final trainin and tset set------')\n",
    "print(confusion_matrix(Y_final_test, Yguess2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preliminary sample dataset, we have 49 true positives and 70 true negative documents correctly classified and the remaining 61 documents are misclassified out of 179 documents.\n",
    "\n",
    "For the first round dataset, we have 500 true positives and 511 true negative total of 1011 documents are correctly classified and 380 documents are misclassified out of 1391 documents.\n",
    "\n",
    "The confusion matrix for the final training and test set presents 821 true positives and 1050 true negative total of 1871 documents correctly classified and 492 documents are misclassified out of 2363 documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different classifiers and check the accuracy for each classifier.\n",
    "DummyClassifier() with  accuracy .........\n",
    "DecisionTreeClassifier() with the  accuracy ..............\n",
    "KNeighborsClassifier() with the accuracy of ............\n",
    "LinearSVC() with accuracy of 0.8017241379310345\n",
    "LinearSVR() .........\n",
    "MLPClassifier(alpha=1) ..........\n",
    "SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5) .........\n",
    "SGDClassifier(loss=\"log\", max_iter=5) ...........\n",
    "and many more classifiers we can add here............\n",
    "\n",
    "LinearRegression()\n",
    "...\n",
    "...\n",
    "Finally, we can choose the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's try a classifier such as MLPClassifier and SGDClassifier(loss=\" log\", max_iter=5) the hyperparameter max_iter will be changed based on the size of the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_document_classifier2(X,Y):\n",
    "    pipeline = make_pipeline(TfidfVectorizer(), \n",
    "                            MLPClassifier(alpha=1)\n",
    "                            )\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score from the second/final round dataset: 0.7735928903935675\n"
     ]
    }
   ],
   "source": [
    "clf_final = train_document_classifier2(X_final_train, Y_final_train)\n",
    "Yguess6 = clf_final.predict(X_final_test)\n",
    "accScore20 = accuracy_score(Y_final_test, Yguess6)\n",
    "print(\"The accuracy score from the second/final round dataset:\", accScore20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_document_classifier1(X,Y):\n",
    "    pipeline = make_pipeline(TfidfVectorizer(), \n",
    "                            SGDClassifier(loss=\"log\", max_iter=5)\n",
    "                            )\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mola/.local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 got score: 0.7867118070249682\n",
      "Current average: 0.7867118070249682\n",
      "############################\n",
      "Run 2 got score: 0.781633516716039\n",
      "Current average: 0.7841726618705036\n",
      "############################\n",
      "Run 3 got score: 0.7833262801523487\n",
      "Current average: 0.7838905346311186\n",
      "############################\n",
      "Run 4 got score: 0.7867118070249682\n",
      "Current average: 0.784595852729581\n",
      "############################\n",
      "Run 5 got score: 0.7875581887431231\n",
      "Current average: 0.7851883199322894\n",
      "############################\n",
      "-----Total Result-----\n",
      "Final Result:  0.7851883199322894\n"
     ]
    }
   ],
   "source": [
    "number_of_test = 5\n",
    "acc_test = 0\n",
    "for i in range(number_of_test):\n",
    "    \n",
    "    clf_commentss = train_document_classifier1(X_final_train, Y_final_train)\n",
    "    Yguess4 = clf_commentss.predict(X_final_test)\n",
    "    accScore = accuracy_score(Y_final_test, Yguess4)\n",
    "    print(\"Run\", (i+1), \"got score:\", accScore)\n",
    "    acc_test += accScore\n",
    "    print(\"Current average:\", acc_test/(i+1))\n",
    "    print(\"############################\")\n",
    "print('-----Total Result-----')\n",
    "print(\"Final Result: \", acc_test/number_of_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the SGDClassifier is more than Linear SVC. Therefore, we chose SGDClassifier as the best classifier for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can apply to use the final test set for the final training set and calculate the accuracy of the training set using the given test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the final training set \n",
    "Xtrain = df_traing['comments']\n",
    "Ytrain= df_traing['target'].astype(int)\n",
    "# For the final test set\n",
    "Xtest = df_test['comments']\n",
    "Ytest = df_test['target'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the accuracy for the final data set and domain shift affects the performance of a classifier.\n",
    "Let's see how well the classifier perform on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for the final dateset: 0.8017241379310345\n"
     ]
    }
   ],
   "source": [
    "clf_final_with_test = train_document_classifier(Xtrain, Ytrain)\n",
    "Yguess_with_test = clf_final_with_test.predict(Xtest)\n",
    "accScore = accuracy_score(Ytest, Yguess_with_test)\n",
    "print(\"The accuracy score for the final dateset:\", accScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the cross-validation and confusion matrix for the last training and test set and check how many are labeled correctly and how many are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Cross_validation for the first round------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mola/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.42187452, 0.33099151, 0.3473568 ]), 'score_time': array([0.12289691, 0.14712071, 0.12762499]), 'test_score': array([0.7813611 , 0.76809754, 0.76047752]), 'train_score': array([0.96113792, 0.96177778, 0.96419048])}\n",
      "-----Confusion Matrix for the final trainin and tset set------\n",
      "[[421 123]\n",
      " [107 509]]\n"
     ]
    }
   ],
   "source": [
    "print('-----Cross_validation for the first round------')\n",
    "print(cross_validate(clf_final_with_test, Xtrain, Ytrain))\n",
    "\n",
    "print('-----Confusion Matrix for the final trainin and tset set------')\n",
    "print(confusion_matrix(Ytest, Yguess_with_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for the final training and test set presents 421 true positives and 509 true negative total of 930 documents correctly classified and 230 documents are misclassified out of 1160 documents(test set document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score slightly increases when we use the test set. we have got 0.7917900973338976 accuracies by splitting the final set to training and test set. But the accuracy of the system using the given test set is 0.8017241379310345."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
